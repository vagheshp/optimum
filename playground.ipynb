{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-classification\n",
    "\n",
    "[REF](https://github.com/pytorch/ort/blob/3c14f36cdacb15ed0de7ba8559e827e5004c7b1f/torch_ort_inference/torch_ort/ortinferencemodule/ortinferencemodule.py#L109)\n",
    "```python\n",
    "self._device = _utils_infer.get_device_from_module(module)\n",
    "\n",
    "\n",
    "run_options = C.RunOptions()\n",
    "\n",
    "# Pre-process inputs to make them compatible with onnxruntime\n",
    "onnx_input_names = [inp.name for inp in self._onnx_models.exported_model.graph.input]\n",
    "inputs = _utils_infer.get_user_inputs(onnx_input_names, self._flattened_module._input_info, inputs, kwargs, self._device)\n",
    "\n",
    "io_binding = self._inference_session.io_binding()\n",
    "_utils._create_iobinding(io_binding, inputs, self._onnx_models.exported_model, self._device)\n",
    "\n",
    "# Run inference session\n",
    "self._inference_session.run_with_iobinding(io_binding, run_options)\n",
    "\n",
    "# Post-process outputs to make them compatible with pytorch\n",
    "forward_outputs = io_binding._iobinding.get_outputs()\n",
    "\n",
    "user_outputs = _utils._ortvalues_to_torch_tensor(forward_outputs, self._device)\n",
    "return _io.unflatten_user_output(self._module_output_schema, user_outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new utils\n",
    "\n",
    "_need onnxruntime-training_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting onnxruntime\n",
      "  Using cached onnxruntime-1.12.0-cp39-cp39-manylinux_2_27_x86_64.whl (4.9 MB)\n",
      "Collecting onnxruntime-training\n",
      "  Using cached onnxruntime_training-1.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (88.8 MB)\n",
      "Requirement already satisfied: protobuf in /home/ubuntu/.local/lib/python3.9/site-packages (from onnxruntime) (3.20.1)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.local/lib/python3.9/site-packages (from onnxruntime) (1.10.1)\n",
      "Requirement already satisfied: flatbuffers in /home/ubuntu/.local/lib/python3.9/site-packages (from onnxruntime) (2.0)\n",
      "Requirement already satisfied: coloredlogs in /home/ubuntu/.local/lib/python3.9/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/ubuntu/.local/lib/python3.9/site-packages (from onnxruntime) (1.23.1)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.9/site-packages (from onnxruntime) (21.3)\n",
      "Requirement already satisfied: h5py in /home/ubuntu/.local/lib/python3.9/site-packages (from onnxruntime-training) (3.7.0)\n",
      "Requirement already satisfied: onnx in /home/ubuntu/.local/lib/python3.9/site-packages (from onnxruntime-training) (1.12.0)\n",
      "Requirement already satisfied: cerberus in /home/ubuntu/.local/lib/python3.9/site-packages (from onnxruntime-training) (1.3.4)\n",
      "Requirement already satisfied: setuptools>=41.4.0 in /opt/conda/lib/python3.9/site-packages (from onnxruntime-training) (63.1.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /home/ubuntu/.local/lib/python3.9/site-packages (from onnx->onnxruntime-training) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/.local/lib/python3.9/site-packages (from packaging->onnxruntime) (3.0.9)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.local/lib/python3.9/site-packages (from sympy->onnxruntime) (1.2.1)\n",
      "Installing collected packages: onnxruntime-training, onnxruntime\n",
      "Successfully installed onnxruntime-1.12.0 onnxruntime-training-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime onnxruntime-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.12.0+cu113\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 11.3\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 11030\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [11040]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<onnxruntime.capi.onnxruntime_pybind11_state.OrtValue at 0x7f59c5ab9b30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from onnxruntime.capi.onnxruntime_inference_collection import OrtValue\n",
    "from distutils.version import LooseVersion\n",
    "from torch.utils.dlpack import to_dlpack\n",
    "from onnxruntime.capi import _pybind_state as C # needs onnxruntime-training \n",
    "from torch._C import _from_dlpack\n",
    "\n",
    "def _ortvalue_from_torch_tensor(torch_tensor):\n",
    "    # TODO: Current DLPack doesn't support bool and PyTorch disables converting bool tensor to DLPack in recent commit.\n",
    "    # https://github.com/pytorch/pytorch/blob/7e7be526c9d9179f35084e9cca5b5c5ad5172100/aten/src/ATen/DLConvertor.cpp#L41\n",
    "    # We need to convert bool tensor to unit8 tensor to workaround this.\n",
    "    # DLPack is discussing how to support bool type, we can remove this workaround once both DLPack\n",
    "    # and PyTorch support bool type.\n",
    "    # is_bool_tensor = torch_tensor.dtype == torch.bool\n",
    "    # print(is_bool_tensor)\n",
    "    # if is_bool_tensor and LooseVersion(torch.__version__) >= LooseVersion(\"1.10.0\"):\n",
    "    #     torch_tensor = torch_tensor.to(torch.uint8)\n",
    "    print(torch_tensor.device)\n",
    "    # if torch_tensor.device.type == \"ort\":\n",
    "    #     print(\"ort device\")\n",
    "    #     return C.aten_ort_tensor_to_ort_value(torch_tensor)\n",
    "    return C.OrtValue.from_dlpack(to_dlpack(torch_tensor), False)\n",
    "\n",
    "def _ortvalues_to_torch_tensor(ortvalues, device):\n",
    "    # if len(ortvalues) == 0:\n",
    "    #     return []\n",
    "\n",
    "    # if \"ort\" == device.type:\n",
    "    #     if not hasattr(C, \"to_aten_ort_device_tensor\"):\n",
    "    #         raise AttributeError(\"onnxruntime is missing to_aten_ort_device_tensor needed to support device == 'ort'.\")\n",
    "    #     return [C.to_aten_ort_device_tensor(ov) for ov in ortvalues]\n",
    "\n",
    "    # if not isinstance(ortvalues, C.OrtValueVector):\n",
    "    #     raise TypeError(\"ortvalues must be an instance of OrtValueVector not %r.\" % type(ortvalues))\n",
    "\n",
    "    res = ortvalues.to_dlpacks(_from_dlpack)\n",
    "    # bool_indices = ortvalues.bool_tensor_indices()\n",
    "    # if len(bool_indices):\n",
    "    #     # DLPack structure does not know for sure if it stores boolean\n",
    "    #     # or uint8. Method to_dlpacks cannot be used in that case.\n",
    "    #     # Signature of *dl_packs* is `to_dlpacks(dlp, fct) -> list[torch.Tensor]`.\n",
    "    #     # And fct is a function with signature `fct(dlp) -> torch.Tensor`.\n",
    "    #     # Boolean tensors are converted into uint8 tensor with the DLPack protocol.\n",
    "    #     # Therefore, the function `fct` does not know if the dlpack structure\n",
    "    #     # is a boolean tensor or a uint8 tensor.\n",
    "    #     # We could either consider another function as an input in\n",
    "    #     # `to_dlpacks` or add an argument to `fct(dlp, ortvalue)`.\n",
    "    #     # Second option makes it impossible to directly use `_from_dlpack` or\n",
    "    #     # or `from_dlpack` from torch.\n",
    "    #     # The best option would be to add boolean type in DLDataTypeCode.\n",
    "    #     for i in range(0, len(bool_indices)):\n",
    "    #         j = bool_indices[i]\n",
    "    #         res[j] = res[j].to(torch.bool)\n",
    "\n",
    "    return res\n",
    "\n",
    "def get_device_index(device):\n",
    "    if isinstance(device, str):\n",
    "        # could be 'cuda:0', 'cuda:1', or 'cpu'. with cpu, set index=0\n",
    "        device = torch.device(device)\n",
    "    elif isinstance(device, int):\n",
    "        return device\n",
    "    return 0 if device.index is None else device.index\n",
    "\n",
    "\n",
    "_ortvalue_from_torch_tensor(torch.ones(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = ORTModelForSequenceClassification.from_pretrained(\"optimum/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"optimum/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "payload = \"I hate you\"\n",
    "\n",
    "d = tokenizer(payload, return_tensors=\"pt\")\n",
    "\n",
    "io_binding = model.model.io_binding()\n",
    "# inputs\n",
    "for key,val in d.items():\n",
    "  io_binding.bind_ortvalue_input(key, OrtValue(_ortvalue_from_torch_tensor(val)))\n",
    "# outputs\n",
    "for name in list(model.model_outputs.keys()):\n",
    "    io_binding.bind_output(name, model.device.type, device_id=model.device.index)\n",
    "model.model.run_with_iobinding(io_binding)\n",
    "# Copy output contents to CPU (if on another device). No-op if already on the CPU.\n",
    "# Y = io_binding.copy_outputs_to_cpu()[0]\n",
    "raw_outputs= io_binding._iobinding.get_outputs()\n",
    "outputs = _ortvalues_to_torch_tensor(raw_outputs, model.device)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-3.7966,  4.0482]])]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload = \"I hate you. I hate you. But i like you\"\n",
    "\n",
    "d = tokenizer(payload, return_tensors=\"pt\")\n",
    "\n",
    "io_binding = model.model.io_binding()\n",
    "# inputs\n",
    "for key,val in d.items():\n",
    "  io_binding.bind_ortvalue_input(key, OrtValue(_ortvalue_from_torch_tensor(val)))\n",
    "# outputs\n",
    "for name in list(model.model_outputs.keys()):\n",
    "    io_binding.bind_output(name, model.device.type, device_id=get_device_index(model.device))\n",
    "model.model.run_with_iobinding(io_binding)\n",
    "# Copy output contents to CPU (if on another device). No-op if already on the CPU.\n",
    "# Y = io_binding.copy_outputs_to_cpu()[0]\n",
    "raw_outputs= io_binding._iobinding.get_outputs()\n",
    "outputs = _ortvalues_to_torch_tensor(raw_outputs, model.device)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class IOBindingModel(ORTModelForSequenceClassification):\n",
    "    def __init__(self, model=None, config=None, **kwargs):\n",
    "        super().__init__(model, config, **kwargs)\n",
    "        # create {name:idx} dict for model outputs\n",
    "        self.model_outputs = {output_key.name: idx for idx, output_key in enumerate(self.model.get_outputs())}\n",
    "        self.model_inputs = {output_key.name: idx for idx, output_key in enumerate(self.model.get_inputs())}\n",
    "        self.model_input_names = list(self.model_inputs.keys())\n",
    "        self.model_output_names = list(self.model_outputs.keys())\n",
    "        self.run_options = C.RunOptions()\n",
    "        # leads to Segmentation fault (core dumped)\n",
    "        # self.io_binding = self.model.io_binding()\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.io_binding = self.model.io_binding()\n",
    "\n",
    "        # add input io binding\n",
    "        self.io_binding.bind_ortvalue_input(\"input_ids\", OrtValue(_ortvalue_from_torch_tensor(input_ids)))\n",
    "        self.io_binding.bind_ortvalue_input(\"attention_mask\", OrtValue(_ortvalue_from_torch_tensor(attention_mask)))\n",
    "        if token_type_ids is not None:\n",
    "            self.io_binding.bind_ortvalue_input(\n",
    "                \"token_type_ids\", OrtValue(_ortvalue_from_torch_tensor(token_type_ids))\n",
    "            )\n",
    "\n",
    "        # add output io binding\n",
    "        for name in self.model_output_names:\n",
    "            self.io_binding.bind_output(name, self.device.type, device_id=self.device.index)\n",
    "\n",
    "        # run inference with binding\n",
    "        self.model.run_with_iobinding(self.io_binding, self.run_options)\n",
    "        # Copy output contents to CPU (if on another device). No-op if already on the CPU.\n",
    "        # Y = io_binding.copy_outputs_to_cpu()[0]\n",
    "        raw_outputs = self.io_binding._iobinding.get_outputs()\n",
    "        outputs = _ortvalues_to_torch_tensor(raw_outputs, self.device)\n",
    "\n",
    "        # clear\n",
    "        # self.io_binding.clear_binding_inputs()\n",
    "        # self.io_binding.clear_binding_outputs()\n",
    "        return SequenceClassifierOutput(logits=outputs[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab970c817a542e2b8a00d0ebc4ee7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/710 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a54b50f8374261b18a196caceea842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 5223, 2017,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = ORTModelForSequenceClassification.from_pretrained(\"optimum/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"optimum/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "payload = \"I hate you\"\n",
    "\n",
    "d = tokenizer(payload, return_tensors=\"pt\")\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011d0ad61bc74dd5be08e630c2468191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/710 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae328cc2ec4145b2ab16c7e59c7d5701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "io_model = IOBindingModel.from_pretrained(\"optimum/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "io_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 5223, 2017, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# payload = \"I hate you.\"\n",
    "palyoad = \"I love you so much. It is incredible what a good person you are. I love you so much. It is incredible what a good person you are.\"\n",
    "\n",
    "d = tokenizer(payload, return_tensors=\"pt\")\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.2 ms ± 153 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit io_model(**d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.4 ms ± 2.34 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model(**d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.9981, -3.2588]])\n",
      "tensor([[ 3.9981, -3.2588]])\n"
     ]
    }
   ],
   "source": [
    "io = io_model(**d)\n",
    "van = model(**d)\n",
    "print(io.logits)\n",
    "print(van.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np\n",
    "\n",
    "def benchmark(seq_len,model,tokenizer,device,iterations=200):\n",
    "    # prepare date\n",
    "    seq_len = \"l \" * (seq_len - 2)\n",
    "    payload = tokenizer(seq_len, return_tensors=\"pt\")\n",
    "    payload = {key:val.to(device) for key,val in payload.items()}\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        _ = model(**payload)\n",
    "    # Timed run\n",
    "    for _ in range(iterations):\n",
    "        start_time = perf_counter()\n",
    "        _ =  model(**payload)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
    "    return {\"seq_len\":payload[\"input_ids\"].shape[1],\"time_avg_ms\":time_avg_ms,\"time_p95_ms\":time_p95_ms}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "seq_lengths=[8,16,32,64,128,256,512]\n",
    "res =[]\n",
    "for seq_len in seq_lengths:\n",
    "    io = benchmark(seq_len,io_model,tokenizer)\n",
    "    res.append({**io,\"model\":\"io\"})\n",
    "    \n",
    "    vanilla = benchmark(seq_len,model,tokenizer)\n",
    "    res.append({**vanilla,\"model\":\"vanilla\"})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_len</th>\n",
       "      <th>model</th>\n",
       "      <th>time_avg_ms</th>\n",
       "      <th>time_p95_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>io</td>\n",
       "      <td>21.845188</td>\n",
       "      <td>31.285430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>19.478945</td>\n",
       "      <td>25.687145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>io</td>\n",
       "      <td>21.550402</td>\n",
       "      <td>22.859014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>21.397938</td>\n",
       "      <td>21.801143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>io</td>\n",
       "      <td>27.146151</td>\n",
       "      <td>28.067425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>27.092843</td>\n",
       "      <td>28.878892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>io</td>\n",
       "      <td>37.805838</td>\n",
       "      <td>38.701186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>37.629447</td>\n",
       "      <td>38.756979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>128</td>\n",
       "      <td>io</td>\n",
       "      <td>61.262763</td>\n",
       "      <td>62.647250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>128</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>61.089302</td>\n",
       "      <td>62.189090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>256</td>\n",
       "      <td>io</td>\n",
       "      <td>120.672068</td>\n",
       "      <td>124.155692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>256</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>120.531309</td>\n",
       "      <td>123.187750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>512</td>\n",
       "      <td>io</td>\n",
       "      <td>268.554570</td>\n",
       "      <td>272.223254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>512</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>268.906273</td>\n",
       "      <td>273.780362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seq_len    model  time_avg_ms  time_p95_ms\n",
       "0         8       io    21.845188    31.285430\n",
       "1         8  vanilla    19.478945    25.687145\n",
       "2        16       io    21.550402    22.859014\n",
       "3        16  vanilla    21.397938    21.801143\n",
       "4        32       io    27.146151    28.067425\n",
       "5        32  vanilla    27.092843    28.878892\n",
       "6        64       io    37.805838    38.701186\n",
       "7        64  vanilla    37.629447    38.756979\n",
       "8       128       io    61.262763    62.647250\n",
       "9       128  vanilla    61.089302    62.189090\n",
       "10      256       io   120.672068   124.155692\n",
       "11      256  vanilla   120.531309   123.187750\n",
       "12      512       io   268.554570   272.223254\n",
       "13      512  vanilla   268.906273   273.780362"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(res)\n",
    "\n",
    "df[[\"seq_len\",\"model\",\"time_avg_ms\",\"time_p95_ms\"]].groupby([\"seq_len\",\"model\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "chart_df = pd.merge(df[df.model == 'io'][[\"seq_len\",\"time_p95_ms\"]], df[df.model == 'vanilla'][[\"seq_len\",\"time_p95_ms\"]], on='seq_len')\n",
    "chart_df = chart_df.rename(columns={\"time_p95_ms_x\": \"io_95\", \"time_p95_ms_y\": \"vanilla_p95\"})\n",
    "plt = chart_df.plot(x=\"seq_len\",y=[\"io_95\",\"vanilla_p95\"],kind=\"line\")\n",
    "plt.figure.savefig('cpu_res.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/bin/bash: -c: line 1: syntax error near unexpected token `cpu_res.png'\n",
      "/bin/bash: -c: line 1: `[cpu_res.png](cpu_res.png)'\n"
     ]
    }
   ],
   "source": [
    "![cpu_res.png](cpu_res.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mWARNING: Skipping onnxruntime as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping onnxruntime-gpu as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: onnxruntime-training 1.11.1+cu113\n",
      "Uninstalling onnxruntime-training-1.11.1+cu113:\n",
      "  Successfully uninstalled onnxruntime-training-1.11.1+cu113\n",
      "Found existing installation: torch 1.11.0+cu113\n",
      "Uninstalling torch-1.11.0+cu113:\n",
      "  Successfully uninstalled torch-1.11.0+cu113\n",
      "Found existing installation: optimum 1.3.0\n",
      "Uninstalling optimum-1.3.0:\n",
      "  Successfully uninstalled optimum-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall onnxruntime onnxruntime-gpu onnxruntime-training torch optimum -y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the combination below works!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: onnx==1.11.0 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (1.11.0)\n",
      "Requirement already satisfied: ninja in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (1.10.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from onnx==1.11.0) (1.23.1)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from onnx==1.11.0) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from onnx==1.11.0) (4.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.11.0+cu113\n",
      "  Using cached https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp39-cp39-linux_x86_64.whl (1637.0 MB)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from torch==1.11.0+cu113) (4.3.0)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.11.0+cu113\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in links: https://download.onnxruntime.ai/onnxruntime_stable_cu113.html\n",
      "Collecting onnxruntime-training==1.12.0+cu113\n",
      "  Downloading https://download.onnxruntime.ai/onnxruntime_training-1.12.0%2Bcu113-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from onnxruntime-training==1.12.0+cu113) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from onnxruntime-training==1.12.0+cu113) (1.23.1)\n",
      "Requirement already satisfied: onnx in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from onnxruntime-training==1.12.0+cu113) (1.11.0)\n",
      "Requirement already satisfied: setuptools>=41.4.0 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from onnxruntime-training==1.12.0+cu113) (63.2.0)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from onnxruntime-training==1.12.0+cu113) (3.20.1)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from onnxruntime-training==1.12.0+cu113) (1.10.1)\n",
      "Requirement already satisfied: cerberus in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from onnxruntime-training==1.12.0+cu113) (1.3.4)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from onnxruntime-training==1.12.0+cu113) (21.3)\n",
      "Requirement already satisfied: flatbuffers in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from onnxruntime-training==1.12.0+cu113) (2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from onnx->onnxruntime-training==1.12.0+cu113) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from packaging->onnxruntime-training==1.12.0+cu113) (3.0.9)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from sympy->onnxruntime-training==1.12.0+cu113) (1.2.1)\n",
      "Installing collected packages: onnxruntime-training\n",
      "Successfully installed onnxruntime-training-1.12.0+cu113\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in links: https://download.onnxruntime.ai/onnxruntime_stable_cu113.html\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement onnxruntime-gpu==1.12.0+cu113 (from versions: 1.7.0, 1.8.0, 1.8.1, 1.9.0, 1.10.0, 1.11.0, 1.11.1, 1.12.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for onnxruntime-gpu==1.12.0+cu113\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting optimum\n",
      "  Using cached optimum-1.3.0-py3-none-any.whl\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.18.0 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from optimum) (4.20.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from optimum) (0.8.1)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from optimum) (1.10.1)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from optimum) (21.3)\n",
      "Requirement already satisfied: coloredlogs in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: torch>=1.9 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from optimum) (1.11.0+cu113)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from optimum) (1.23.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->optimum) (4.3.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->optimum) (2.28.1)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->optimum) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->optimum) (6.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->optimum) (3.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from packaging->optimum) (3.0.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from transformers[sentencepiece]>=4.18.0->optimum) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from transformers[sentencepiece]>=4.18.0->optimum) (0.12.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from transformers[sentencepiece]>=4.18.0->optimum) (0.1.96)\n",
      "Requirement already satisfied: protobuf<=3.20.1 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from transformers[sentencepiece]>=4.18.0->optimum) (3.20.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from sympy->optimum) (1.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->optimum) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->optimum) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->optimum) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/.conda/envs/ort/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->optimum) (1.26.10)\n",
      "Installing collected packages: optimum\n",
      "Successfully installed optimum-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# it seems 1.12.0 doesn't work with cuda11.3\n",
    "# Inconsistency detected by ld.so: dl-version.c: 205: _dl_check_map_versions: Assertion `needed != NULL' failed!\n",
    "%pip install onnx==1.11.0 ninja\n",
    "%pip install torch==1.11.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "%pip install onnxruntime-training==1.12.0+cu113 -f https://download.onnxruntime.ai/onnxruntime_stable_cu113.html\n",
    "%pip install onnxruntime-gpu==1.12.0+cu113 -f https://download.onnxruntime.ai/onnxruntime_stable_cu113.html\n",
    "%pip install optimum matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime import get_available_providers, get_device\n",
    "import onnxruntime.training\n",
    "import onnxruntime \n",
    "\n",
    "# check available providers\n",
    "assert 'CUDAExecutionProvider' in get_available_providers(), \"ONNX Runtime GPU provider not found. Make sure onnxruntime-gpu is installed and onnxruntime is uninstalled.\"\n",
    "assert \"GPU\" == get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manuall test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 710/710 [00:00<00:00, 734kB/s]\n",
      "Downloading: 100%|██████████| 268M/268M [00:02<00:00, 103MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model device cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "<onnxruntime.capi.onnxruntime_pybind11_state.OrtValueVector object at 0x7f5ae8017c70>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[ 3.8724, -3.1543]], device='cuda:0')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from onnxruntime.capi.onnxruntime_inference_collection import OrtValue\n",
    "import torch \n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = ORTModelForSequenceClassification.from_pretrained(\"optimum/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"optimum/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model.to(device)\n",
    "print(f\"model device {model.device}\")\n",
    "\n",
    "\n",
    "payload = \"I hate you\"\n",
    "\n",
    "d = tokenizer(payload, return_tensors=\"pt\")\n",
    "\n",
    "io_binding = model.model.io_binding()\n",
    "# inputs\n",
    "for key,val in d.items():\n",
    "  io_binding.bind_ortvalue_input(key, OrtValue(_ortvalue_from_torch_tensor(val.to(device))))\n",
    "# outputs\n",
    "for name in list(model.model_outputs.keys()):\n",
    "    io_binding.bind_output(name, model.device.type, device_id=model.device.index)\n",
    "model.model.run_with_iobinding(io_binding)\n",
    "# Copy output contents to CPU (if on another device). No-op if already on the CPU.\n",
    "# Y = io_binding.copy_outputs_to_cpu()[0]\n",
    "raw_outputs= io_binding._iobinding.get_outputs()\n",
    "print(raw_outputs)\n",
    "outputs = _ortvalues_to_torch_tensor(raw_outputs, model.device)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 710/710 [00:00<00:00, 389kB/s]\n",
      "Downloading: 100%|██████████| 268M/268M [00:02<00:00, 93.3MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model device cuda:0\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import torch \n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = ORTModelForSequenceClassification.from_pretrained(\"optimum/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"optimum/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"model device {model.device}\")\n",
    "payload = \"I hate you\"\n",
    "\n",
    "d = tokenizer(payload, return_tensors=\"pt\")\n",
    "# model(**d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 710/710 [00:00<00:00, 390kB/s]\n",
      "Downloading: 100%|██████████| 268M/268M [00:02<00:00, 96.2MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "io_model = IOBindingModel.from_pretrained(\"optimum/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "io_model.to(device)\n",
    "io_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[\"input_ids\"]= d[\"input_ids\"].to(device)\n",
    "d[\"attention_mask\"]= d[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "io_model(**d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 710/710 [00:00<00:00, 691kB/s]\n",
      "Downloading: 100%|██████████| 268M/268M [00:03<00:00, 78.1MB/s] \n",
      "Downloading: 100%|██████████| 710/710 [00:00<00:00, 440kB/s]\n",
      "Downloading: 100%|██████████| 268M/268M [00:02<00:00, 96.0MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.IOBindingModel at 0x7fa7bb252d30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = ORTModelForSequenceClassification.from_pretrained(\"optimum/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "io_model = IOBindingModel.from_pretrained(\"optimum/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"optimum/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "model.to(device)\n",
    "io_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "seq_lengths=[8,16,32,64,128,256,512]\n",
    "res =[]\n",
    "for seq_len in seq_lengths:\n",
    "    print(\"seq_len: \",seq_len)\n",
    "    io = benchmark(seq_len,io_model,tokenizer,device,iterations=500)\n",
    "    res.append({**io,\"model\":\"io\",\"norm_p95\":io[\"time_p95_ms\"]/(seq_len)})\n",
    "    \n",
    "    vanilla = benchmark(seq_len,model,tokenizer,device,iterations=500)\n",
    "    res.append({**vanilla,\"model\":\"vanilla\",\"norm_p95\":io[\"time_p95_ms\"]/(seq_len)})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res)\n",
    "\n",
    "chart_df = pd.merge(df[df.model == 'io'][[\"seq_len\",\"time_p95_ms\"]], df[df.model == 'vanilla'][[\"seq_len\",\"time_p95_ms\"]], on='seq_len')\n",
    "chart_df = chart_df.rename(columns={\"time_p95_ms_x\": \"io_p95\", \"time_p95_ms_y\": \"vanilla_p95\"})\n",
    "plt = chart_df.plot(x=\"seq_len\",y=[\"io_p95\",\"vanilla_p95\"],kind=\"line\")\n",
    "plt.figure.savefig('gpu_res.png', dpi=300)\n",
    "\n",
    "chart_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d3663dc617162e6f411b3d1134944c718d8d278cbddf84fb85a62171cfda1e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ort')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
